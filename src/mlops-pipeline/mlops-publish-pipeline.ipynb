{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Responsible AI MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Read [Azure Machine Learning Pipelines](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-ml-pipelines) overview, or the [readme article](../README.md) on Azure Machine Learning Pipelines to get more information.\n",
    " \n",
    "\n",
    "This notebook shows the construction of a machine learning service **pipeline** with MLOps techniques that runs jobs unattended in different compute clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Azure Machine Learning Basics\n",
    "\n",
    "Please, before anything set up with a working config file that has information on your workspace, subscription id, etc located on:\n",
    "\n",
    "- './notebooks/notebook-settings/config.json' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import azureml.core\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.pipeline.core import PipelineEndpoint\n",
    "from azureml.pipeline.core import PipelineData, TrainingOutput\n",
    "from azureml.core import Workspace, Run, Experiment, Datastore, Dataset\n",
    "from azureml.core.runconfig import RunConfiguration, CondaDependencies\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline-specific SDK imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we import key pipeline modules, whose use will be illustrated in the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import DataType\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, StepSequence, TrainingOutput\n",
    "from azureml.pipeline.steps import PythonScriptStep, AutoMLStep\n",
    "from azureml.pipeline.core import PublishedPipeline, PipelineRun\n",
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "\n",
    "print(\"Pipeline SDK-specific imports completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to connect to your workspace using the Azure ML SDK.\n",
    "\n",
    "Note: If the authenticated session with your Azure subscription has expired since you completed the previous exercise, you'll be prompted to reauthenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(\"../notebooks-settings/config.json\")\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Default datastore (Azure Blob storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datastore concepts\n",
    "A [Datastore](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore(class) is a place where data can be stored that is then made accessible to a compute either by means of mounting or copying the data to the compute target. \n",
    "\n",
    "A Datastore can either be backed by an Azure File Storage (default) or by an Azure Blob Storage.\n",
    "\n",
    "In this next step, we will upload the training and test set into the workspace's default storage (File storage), and another piece of data to Azure Blob Storage. When to use [Azure Blobs](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction), [Azure Files](https://docs.microsoft.com/en-us/azure/storage/files/storage-files-introduction), or [Azure Disks](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/managed-disks-overview) is [detailed here](https://docs.microsoft.com/en-us/azure/storage/common/storage-decide-blobs-files-disks).\n",
    "\n",
    "**Please take good note of the concept of the datastore.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will get Blob storage associated with the workspace.\n",
    "\n",
    "The following call GETS the Azure Blob Store associated with your workspace.\n",
    "\n",
    "Note that workspaceblobstore is **the name of this store and CANNOT BE CHANGED and must be used as is**\n",
    "\n",
    "The above call is equivalent to Datastore(ws, \"workspaceblobstore\") or simply Datastore(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required data and script files for the the tutorial\n",
    "Sample files required to finish this tutorial are already copied to the project folder specified above. Even though the .py provided in the samples don't have much \"ML work,\" as a data scientist, you will work on this extensively as part of your work. To complete this tutorial, the contents of these files are not very important. The one-line files are for demostration purpose only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to default datastore\n",
    "Default datastore on workspace is the Azure  File storage. The workspace has a Blob storage associated with it as well. Let's upload a file to each of these storages. \n",
    "\n",
    "get_default_datastore() gets the default Azure File Store associated with your workspace. Here we are reusing the def_file_store object we obtained earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of upload_dataset:\n",
    "\n",
    "1. Workspace object\n",
    "2. Blob Storage Datastore object\n",
    "3. Azure Dataset name\n",
    "4. Local path on Datastore\n",
    "5. Local path of dataset\n",
    "6. This dataset will be use on Datadrift Detector (True/False)\n",
    "7. dataset type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we'll upload:\n",
    "\n",
    "1. The complete dataset about parkinson patients. This dataset contain information about parkinson relevent factors to predict if the patient need a treatment or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_dataset(ws, def_blob_store, 'parkinson',\n",
    "                                  'parkinson/acc_final_v3_0.csv', \"../../dataset/acc_final_v3_0.csv\",\n",
    "                                  use_datadrift=False, type_dataset=\"Standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_dataset(ws, def_blob_store, 'parkinson',\n",
    "                                  'parkinson/acc_final_v3_1.csv', \"../../dataset/acc_final_v3_1.csv\",\n",
    "                                  use_datadrift=False, type_dataset=\"Standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_dataset(ws, def_blob_store, 'parkinson',\n",
    "                                  'parkinson/acc_final_v3_2.csv', \"../../dataset/acc_final_v3_2.csv\",\n",
    "                                  use_datadrift=False, type_dataset=\"Standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_dataset(ws, def_blob_store, 'parkinson',\n",
    "                                  'parkinson/acc_final_v3_3.csv', \"../../dataset/acc_final_v3_3.csv\",\n",
    "                                  use_datadrift=False, type_dataset=\"Standard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) See your files using Azure Portal\n",
    "Once you successfully uploaded the files, you can browse to them (or upload more files) using [Azure Portal](https://portal.azure.com). At the portal, make sure you have selected your subscription (click *Resource Groups* and then select the subscription). Then look for your **Machine Learning Workspace** (it has your *alias* as the name). It has a link to your storage. Click on the storage link. It will take you to a page where you can see [Blobs](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction), [Files](https://docs.microsoft.com/en-us/azure/storage/files/storage-files-introduction), [Tables](https://docs.microsoft.com/en-us/azure/storage/tables/table-storage-overview), and [Queues](https://docs.microsoft.com/en-us/azure/storage/queues/storage-queues-introduction). We have just uploaded a file to the Blob storage and another one to the File storage. You should be able to see both of these files in their respective locations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Targets\n",
    "A compute target specifies where to execute your program such as a remote Docker on a VM, or a cluster. A compute target needs to be addressable and accessible by you.\n",
    "\n",
    "**You need at least one compute target to send your payload to. We are planning to use Azure Machine Learning Compute exclusively for this tutorial for all steps. However in some cases you may require multiple compute targets as some steps may run in one compute target like Azure Machine Learning Compute, and some other steps in the same pipeline could run in a different compute target.**\n",
    "\n",
    "*The example belows show creating/retrieving/attaching to an Azure Machine Learning Compute instance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Compute Targets on the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cts = ws.compute_targets\n",
    "for ct in cts:\n",
    "    print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve or create a Azure Machine Learning compute\n",
    "Azure Machine Learning Compute is a service for provisioning and managing clusters of Azure virtual machines for running machine learning workloads. Let's create a new Azure Machine Learning Compute in the current workspace, if it doesn't already exist. We will then run the training script on this compute target.\n",
    "\n",
    "If we could not find the compute with the given name in the previous cell, then we will create a new compute here. We will create an Azure Machine Learning Compute containing **STANDARD_D2_V2 CPU VMs**. This process is broken down into the following steps:\n",
    "\n",
    "1. Create the configuration\n",
    "2. Create the Azure Machine Learning compute\n",
    "\n",
    "**This process will take about 3 minutes and is providing only sparse output in the process. Please make sure to wait until the call returns before moving to the next cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_compute_name = \"aml-compute\"\n",
    "vm_size = \"STANDARD_DS3_V2\"\n",
    "aml_compute = get_compute_aml(ws, aml_compute_name, vm_size)\n",
    "print(\"Azure Machine Learning Compute attached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MLOps Pipeline Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines consist of one or more steps, which can be Python scripts, or specialized steps like an Auto ML training estimator or a data transfer step that copies data from one location to another. Each step can run in its own compute context.\n",
    "\n",
    "In this notebook, you'll build a Responsible AI pipeline that contains nine steps, which include the following steps:\n",
    "\n",
    "1. **Exploratory Analysis and Preprocessing**\n",
    "2. **AutoML Training/Evaluation**\n",
    "3. **Register AutoML Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline steps run configs\n",
    "\n",
    "In this notebook, you'll use the same compute for both steps, but it's important to realize that each step is run independently; so you could specify different compute contexts for each step if appropriate.\n",
    "\n",
    "First, get the compute target you created in a previous cells (if it doesn't exist, it will be created).\n",
    "Each step of the pipeline will need some dependencies to execute and generate well results. So, because of this need, we break down the run configurations and focus on each step and see what dependencies each needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Config - Analysis and preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_preprocessing_config = RunConfiguration(conda_dependencies=CondaDependencies.create(\n",
    "        conda_packages=['numpy==1.18.1', 'pandas==1.0.4'],\n",
    "        pip_packages=['azureml-sdk', 'matplotlib==3.1.3', 'seaborn==0.10.0', 'sklearn==0.0', 'pandas-profiling==2.8.0'])\n",
    "    )\n",
    "run_preprocessing_config.environment.docker.enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Config - Register Model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_automl_config = RunConfiguration(conda_dependencies=CondaDependencies.create(\n",
    "        conda_packages=[],\n",
    "        pip_packages=['azureml-sdk', 'azureml-train-automl-runtime==1.8.0.post1', 'xgboost==1.1.1'])\n",
    "    )\n",
    "run_automl_config.environment.docker.enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Steps in a Pipeline\n",
    "A Step is a unit of execution. Step typically needs a target of execution (compute target), a script to execute, and may require script arguments and inputs, and can produce outputs. The step also could take a number of other parameters. Azure Machine Learning Pipelines provides the following built-in Steps:\n",
    "\n",
    "- [**PythonScriptStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep?view=azure-ml-py): Add a step to run a Python script in a Pipeline.\n",
    "\n",
    "- [**AutoML Step**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.automl_step.automlsteprun?view=azure-ml-py): Creates a AutoML step to manage, check status, and retrieve run details once an automated ML run is submitted in a pipeline.\n",
    "\n",
    "- [**AdlaStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.adla_step.adlastep?view=azure-ml-py): Adds a step to run U-SQL script using Azure Data Lake Analytics.\n",
    "\n",
    "- [**DataTransferStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.data_transfer_step.datatransferstep?view=azure-ml-py): Transfers data between Azure Blob and Data Lake accounts.\n",
    "\n",
    "- [**DatabricksStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.databricks_step.databricksstep?view=azure-ml-py): Adds a DataBricks notebook as a step in a Pipeline.\n",
    "\n",
    "- [**HyperDriveStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.hyper_drive_step.hyperdrivestep?view=azure-ml-py): Creates a Hyper Drive step for Hyper Parameter Tuning in a Pipeline.\n",
    "\n",
    "The following code will create a PythonScriptStep to be executed in the Azure Machine Learning Compute we created above using train.py, one of the files already made available in the project folder.\n",
    "\n",
    "A **PythonScriptStep** is a basic, built-in step to run a Python Script on a compute target. It takes a script name and optionally other parameters like arguments for the script, compute target, inputs and outputs. If no compute target is specified, default compute target for the workspace is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure PipelineData\n",
    "\n",
    "Here, we configure some outputs on the pipeline. The first output (preprocess_step_output) will be between **Exploratory Analysis and Preprocessing** and **AutoML Training/Evaluation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_step_output = PipelineData(\"hd_preprocessed\", datastore=def_blob_store).as_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Dataset Columns to PipelineDataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = convert_dataset_columns(ws,'./scripts/schema_dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Parameters Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppb = PipelineParameterBuilder(\"../utils/params_config/pipeline_parameters.json\")\n",
    "ppb.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure AutoML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"max_concurrent_iterations\": 3,\n",
    "    \"primary_metric\" : 'AUC_weighted',\n",
    "    \"featurization\": 'auto',\n",
    "    'model_explainability': True,\n",
    "    'iterations': 1,\n",
    "    'validation_size': 0.3,\n",
    "    'enable_early_stopping': True,\n",
    "    'label_column_name': 'Tremor',\n",
    "    'enable_onnx_compatible_models': True,\n",
    "    'task': 'classification'\n",
    "}\n",
    "automl_config = AutoMLConfig(compute_target=aml_compute,\n",
    "                             training_data = preprocess_step_output.parse_delimited_files(set_column_types=data_dict),\n",
    "                             path = \".\",\n",
    "                             **automl_settings\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_output_name = 'metrics_output'\n",
    "best_model_output_name = 'best_model_output'\n",
    "\n",
    "metrics_data = PipelineData(name='metrics_data',\n",
    "                           datastore=def_blob_store,\n",
    "                           pipeline_output_name=metrics_output_name,\n",
    "                           training_output=TrainingOutput(type='Metrics'))\n",
    "model_data = PipelineData(name='model_data',\n",
    "                           datastore=def_blob_store,\n",
    "                           pipeline_output_name=best_model_output_name,\n",
    "                           training_output=TrainingOutput(type='Model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Pipeline\n",
    "\n",
    "Now you're ready to create and run a pipeline. First you need to define the steps for the pipeline, and any data references that need to passed between them. The Pipeline Data and Parameters configurations were created in the last cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = './scripts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_folder = '../deployment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_step = PythonScriptStep(name=\"Exploratory Analysis and Preprocessing\",\n",
    "                         script_name=\"preprocessing.py\",\n",
    "                         compute_target=aml_compute, \n",
    "                         source_directory=project_folder,\n",
    "                         arguments=[\n",
    "                              \"--datastore\", ppb.get_pipeline_parameter_obj('datastore_preprocessing_step'),\n",
    "                              \"--dataset_name\", ppb.get_pipeline_parameter_obj('dataset_name_preprocessing_step'),\n",
    "                              \"--dataset_preprocessed_name\", ppb.get_pipeline_parameter_obj('dataset_preprocessed_name_preprocessing_step'),\n",
    "                              \"--output_preprocess_dataset\", preprocess_step_output,\n",
    "                         ],\n",
    "                         outputs=[preprocess_step_output],\n",
    "                         runconfig=run_preprocessing_config,\n",
    "                         allow_reuse=False)\n",
    "\n",
    "automl_step = AutoMLStep(\n",
    "                name='AutoML_training',\n",
    "                automl_config=automl_config,\n",
    "                inputs = [preprocess_step_output],\n",
    "                outputs=[metrics_data, model_data],\n",
    "                passthru_automl_config=False,\n",
    "                allow_reuse=False)\n",
    "\n",
    "register_model_step = PythonScriptStep(script_name=\"register_model.py\",\n",
    "                                  source_directory=project_folder,\n",
    "                                  name=\"Register Model\",\n",
    "                                  arguments=[\"--model_name\", ppb.get_pipeline_parameter_obj('model_name_register_model_step'),\n",
    "                                             \"--model_data\", model_data,\n",
    "                                             \"--metrics_data\", metrics_data,\n",
    "                                             \"--dataset_name\", ppb.get_pipeline_parameter_obj('dataset_name_register_model_step')],\n",
    "                                  inputs=[model_data, metrics_data],\n",
    "                                  compute_target=aml_compute,\n",
    "                                  runconfig=run_automl_config,\n",
    "                                  allow_reuse=False)\n",
    "\n",
    "steps = [preprocessing_step, automl_step,register_model_step]\n",
    "print(\"Steps created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allow reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The flag *allow_reuse* determines whether the step should reuse previous results when run with the same settings/inputs. This flag's default value is *True*; the default is set to *True* because, when inputs and parameters have not changed, we typically do not want to re-run a given pipeline step. \n",
    "\n",
    "If *allow_reuse* is set to *False*, a new run will always be generated for this step during pipeline execution. The *allow_reuse* flag can come in handy in situations where you do *not* want to re-run a pipeline step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the pipeline\n",
    "Once we have the steps (or steps collection), we can build the [pipeline](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py). By deafult, all these steps will run in **parallel** once we submit the pipeline for run.\n",
    "\n",
    "A pipeline is created with a list of steps and a workspace. Submit a pipeline using [submit](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment%28class%29?view=azure-ml-py#submit). When submit is called, a [PipelineRun](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinerun?view=azure-ml-py) is created which in turn creates [StepRun](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.steprun?view=azure-ml-py) objects for each step in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "print (\"Pipeline is built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the pipeline\n",
    "You have the option to [validate](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py#validate) the pipeline prior to submitting for run. The platform runs validation steps such as checking for circular dependencies and parameter checks etc. even if you do not explicitly call validate method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.validate()\n",
    "print(\"Pipeline validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish the pipeline to use via REST\n",
    "Once you are satisfied with the results of your experiment, you may want to publish the pipeline to get a REST endpoint so the pipeline can be invoked later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_training_pipeline = pipeline.publish(name=\"parkinson_pipeline\",\n",
    "                                            description=\"This pipeline train a model to detect parkinson disease\")\n",
    "print(\"The published pipeline ID is {}\".format(published_training_pipeline.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_endpoint = PipelineEndpoint.publish(workspace=ws,\n",
    "                                            name=\"parkinson_demo_v1\",\n",
    "                                            pipeline=published_training_pipeline,\n",
    "                                            description=\"This http pipeline train a model to detect parkinson disease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Responsible AI. 1.8.0",
   "language": "python",
   "name": "heart-disease-mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
